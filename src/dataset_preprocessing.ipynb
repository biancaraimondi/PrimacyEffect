{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "random_state = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for dataset HWU64\n",
    "\n",
    "# Read data from Dataset/NLU-Data-Home-Domain-Annotated-All.csv\n",
    "df = pd.read_csv('../Dataset/NLU-Data-Home-Domain-Annotated-All.csv', delimiter=';')\n",
    "\n",
    "print('Num of samples in dataset:', len(df))\n",
    "\n",
    "# Count the number of unique values in column intent\n",
    "intents = df['intent'].unique()\n",
    "print('Num of different labels in dataset:', len(intents))\n",
    "# for each intent, print the number of questions in the dataset. find also min\n",
    "min_intent = None\n",
    "for intent in intents:\n",
    "    length = len(df[df['intent'] == intent])\n",
    "    if min_intent is None:\n",
    "        min_intent = length\n",
    "    else:\n",
    "        if length < min_intent:\n",
    "            min_intent = length\n",
    "print('Num of minimum samples in each label:', min_intent)\n",
    "\n",
    "# if column answer_normalised is empty, fill it with the value in column answer\n",
    "df['answer_normalised'] = df['answer_normalised'].fillna(df['answer'])\n",
    "\n",
    "# Drop columns userid, answerid, status, answer_annotation, notes, suggested_entities, answer, question\n",
    "df = df.drop(columns=['userid', 'answerid', 'scenario', 'status', 'answer_annotation', 'notes', 'suggested_entities', 'answer', 'question'])\n",
    "\n",
    "# for each intent, select min_intent questions randomly and remove the rest\n",
    "df_balanced = df.groupby('intent').apply(lambda x: x.sample(min_intent, random_state=random_state)).reset_index(drop=True)\n",
    "\n",
    "# add header to the dataset\n",
    "df_balanced.columns = ['label', 'question']\n",
    "\n",
    "# For each label, we will assign a position based on the order of the label in the intents list\n",
    "label_dict = {label: pos for pos, label in enumerate(intents)}\n",
    "labels = df_balanced['label'].tolist()\n",
    "label_positions = [label_dict[label] for label in labels]\n",
    "# add a column 'label_position' to df\n",
    "df_balanced['label_position'] = label_positions\n",
    "\n",
    "display(df_balanced)\n",
    "print('Num of samples in dataset after balancing:', len(df_balanced))\n",
    "\n",
    "# print rows with empty question, empty label or empty label_position\n",
    "print('Num of rows with empty question:', len(df_balanced[df_balanced['question'] == '']))\n",
    "print('Num of rows with empty label:', len(df_balanced[df_balanced['label'] == '']))\n",
    "print('Num of rows with empty label_position:', len(df_balanced[df_balanced['label_position'] == '']))\n",
    "\n",
    "# Save the balanced dataset to HWU64.csv file \n",
    "df_balanced.to_csv('../Dataset/HWU64.csv', index=False)\n",
    "# Save the list of labels to HWU64_labels.json file as an array\n",
    "with open('../Dataset/HWU64_labels.json', 'w') as f:\n",
    "    json.dump(intents.tolist(), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for dataset CLINC150\n",
    "\n",
    "# Read data from Dataset/data_full.json\n",
    "data = None\n",
    "with open('../Dataset/data_full.json') as f:\n",
    "    d = json.load(f)\n",
    "\n",
    "train = d.get('train')\n",
    "val = d.get('val')\n",
    "test = d.get('test')\n",
    "\n",
    "# Each element in array is in the form of array [question, intent]. We need to convert it to DataFrame with columns question and intent\n",
    "df_train = pd.DataFrame(train, columns=['question', 'label'])\n",
    "df_val = pd.DataFrame(val, columns=['question', 'label'])\n",
    "df_test = pd.DataFrame(test, columns=['question', 'label'])\n",
    "\n",
    "# for each dataframe, add a column 'source' with value 'train', 'val', 'test' respectively\n",
    "df_train['source'] = 'train'\n",
    "df_val['source'] = 'val'\n",
    "df_test['source'] = 'test'\n",
    "\n",
    "# Concatenate df_train, df_val, df_test to form a single dataframe\n",
    "df = pd.concat([df_train, df_val, df_test], ignore_index=True)\n",
    "# drop column 'source' from df\n",
    "df = df.drop(columns=['source'])\n",
    "# order columns in df as 'label', 'question'\n",
    "df = df[['label', 'question']]\n",
    "\n",
    "# print unique values in column intent\n",
    "intents = df['label'].unique()\n",
    "print('Num of different intents in dataset:', len(intents))\n",
    "print('Num of samples in dataset:', len(df))\n",
    "\n",
    "# For each label, we will assign a position based on the order of the label in the intents list\n",
    "label_dict = {label: pos for pos, label in enumerate(intents)}\n",
    "labels = df['label'].tolist()\n",
    "label_positions = [label_dict[label] for label in labels]\n",
    "# add a column 'label_position' to df\n",
    "df['label_position'] = label_positions\n",
    "\n",
    "display(df)\n",
    "\n",
    "# print rows with empty question, empty label or empty label_position\n",
    "print('Num of rows with empty question:', len(df[df['question'] == '']))\n",
    "print('Num of rows with empty label:', len(df[df['label'] == '']))\n",
    "print('Num of rows with empty label_position:', len(df[df['label_position'] == '']))\n",
    "\n",
    "\n",
    "# Save the balanced dataset to CLINC150.csv file \n",
    "df.to_csv('../Dataset/CLINC150.csv', index=False)\n",
    "# Save the list of labels to CLINC150_labels.json file as an array\n",
    "with open('../Dataset/CLINC150_labels.json', 'w') as f:\n",
    "    json.dump(intents.tolist(), f)\n",
    "\n",
    "# Save a subset of the dataset with all the labels but only 50 samples for each label\n",
    "df_subset = df.groupby('label').apply(lambda x: x.sample(25, random_state=random_state)).reset_index(drop=True)\n",
    "df_subset.to_csv('../Dataset/CLINC150_subset.csv', index=False)\n",
    "with open('../Dataset/CLINC150_subset_labels.json', 'w') as f:\n",
    "    json.dump(intents.tolist(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for dataset BANKING77\n",
    "\n",
    "# Read the dataset from the parquet file\n",
    "banking77 = pd.read_parquet('../Dataset/banking77.parquet')\n",
    "# Get the questions text from each sample contained in the dataset\n",
    "sample_ls = list(banking77['text'])\n",
    "# Get the id of the target label from each sample contained in the dataset, i.e. the position of the target label in the original list of labels\n",
    "target_ls = list(banking77['label'])\n",
    "# Original list of labels\n",
    "label_ls_initial = [\n",
    "    \"activate_my_card\",\n",
    "    \"age_limit\",\n",
    "    \"apple_pay_or_google_pay\",\n",
    "    \"atm_support\",\n",
    "    \"automatic_top_up\",\n",
    "    \"balance_not_updated_after_bank_transfer\",\n",
    "    \"balance_not_updated_after_cheque_or_cash_deposit\",\n",
    "    \"beneficiary_not_allowed\",\n",
    "    \"cancel_transfer\",\n",
    "    \"card_about_to_expire\",\n",
    "    \"card_acceptance\",\n",
    "    \"card_arrival\",\n",
    "    \"card_delivery_estimate\",\n",
    "    \"card_linking\",\n",
    "    \"card_not_working\",\n",
    "    \"card_payment_fee_charged\",\n",
    "    \"card_payment_not_recognised\",\n",
    "    \"card_payment_wrong_exchange_rate\",\n",
    "    \"card_swallowed\",\n",
    "    \"cash_withdrawal_charge\",\n",
    "    \"cash_withdrawal_not_recognised\",\n",
    "    \"change_pin\",\n",
    "    \"compromised_card\",\n",
    "    \"contactless_not_working\",\n",
    "    \"country_support\",\n",
    "    \"declined_card_payment\",\n",
    "    \"declined_cash_withdrawal\",\n",
    "    \"declined_transfer\",\n",
    "    \"direct_debit_payment_not_recognised\",\n",
    "    \"disposable_card_limits\",\n",
    "    \"edit_personal_details\",\n",
    "    \"exchange_charge\",\n",
    "    \"exchange_rate\",\n",
    "    \"exchange_via_app\",\n",
    "    \"extra_charge_on_statement\",\n",
    "    \"failed_transfer\",\n",
    "    \"fiat_currency_support\",\n",
    "    \"get_disposable_virtual_card\",\n",
    "    \"get_physical_card\",\n",
    "    \"getting_spare_card\",\n",
    "    \"getting_virtual_card\",\n",
    "    \"lost_or_stolen_card\",\n",
    "    \"lost_or_stolen_phone\",\n",
    "    \"order_physical_card\",\n",
    "    \"passcode_forgotten\",\n",
    "    \"pending_card_payment\",\n",
    "    \"pending_cash_withdrawal\",\n",
    "    \"pending_top_up\",\n",
    "    \"pending_transfer\",\n",
    "    \"pin_blocked\",\n",
    "    \"receiving_money\",\n",
    "    \"Refund_not_showing_up\",\n",
    "    \"request_refund\",\n",
    "    \"reverted_card_payment?\",\n",
    "    \"supported_cards_and_currencies\",\n",
    "    \"terminate_account\",\n",
    "    \"top_up_by_bank_transfer_charge\",\n",
    "    \"top_up_by_card_charge\",\n",
    "    \"top_up_by_cash_or_cheque\",\n",
    "    \"top_up_failed\",\n",
    "    \"top_up_limits\",\n",
    "    \"top_up_reverted\",\n",
    "    \"topping_up_by_card\",\n",
    "    \"transaction_charged_twice\",\n",
    "    \"transfer_fee_charged\",\n",
    "    \"transfer_into_account\",\n",
    "    \"transfer_not_received_by_recipient\",\n",
    "    \"transfer_timing\",\n",
    "    \"unable_to_verify_identity\",\n",
    "    \"verify_my_identity\",\n",
    "    \"verify_source_of_funds\",\n",
    "    \"verify_top_up\",\n",
    "    \"virtual_card_not_working\",\n",
    "    \"visa_or_mastercard\",\n",
    "    \"why_verify_identity\",\n",
    "    \"wrong_amount_of_cash_received\",\n",
    "    \"wrong_exchange_rate_for_cash_withdrawal\",\n",
    "]\n",
    "\n",
    "label_ls = [label_ls_initial[target] for target in target_ls]\n",
    "\n",
    "# Create df from sample_ls and target_ls with columns 'question' and 'label'\n",
    "df = pd.DataFrame(list(zip(sample_ls, label_ls, target_ls)), columns=['question', 'label', 'label_position'])\n",
    "# order columns in df as 'label', 'question', 'label_position'\n",
    "df = df[['label', 'question', 'label_position']]\n",
    "display(df.head())\n",
    "\n",
    "# print rows with empty question, empty label or empty label_position\n",
    "print('Num of rows with empty question:', len(df[df['question'] == '']))\n",
    "print('Num of rows with empty label:', len(df[df['label'] == '']))\n",
    "print('Num of rows with empty label_position:', len(df[df['label_position'] == '']))\n",
    "\n",
    "# Save the balanced dataset to BANKING77.csv file\n",
    "df.to_csv('../Dataset/BANKING77.csv', index=False)\n",
    "# Save the list of labels to BANKING77_labels.json file as an array\n",
    "with open('../Dataset/BANKING77_labels.json', 'w') as f:\n",
    "    json.dump(label_ls_initial, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".pr_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
